# -*- coding: utf-8 -*-
"""EC4305.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eP7UDTMC3HkoNQ8QmZ7M3r00_g_6wCcX

**Setup**
"""

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline

# regression models
from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.svm import SVR
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score

# classification models
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, roc_auc_score

# unsupervised
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans

"""**Load Data**"""

from google.colab import files
import pandas as pd

uploaded = files.upload()

DATA_PATH = "Cleaned_dataset.csv"
df = pd.read_csv(DATA_PATH)

print("Raw shape:", df.shape)

"""**Regression ML on ln_totalscr92**"""

reg_target = "ln_totalscr92"

reg_features = [
    # class sizes (levels)
    "class_size88", "class_size90", "class_size92",
    # class-size dummies
    "small_class_size88", "small_class_size90", "small_class_size92",
    "regular_class_size88", "regular_class_size90", "regular_class_size92",
    "large_class_size88", "large_class_size90", "large_class_size92",
    # SES
    "high_faminc88", "high_faminc92",
    # demographics (already dummies)
    "male", "asian", "hispanic", "black", "white", "native_american",
    # teacher / job status
    "fulltime88", "fulltime92",
    "certified88", "certified92",
    # always small or not
    "always_small", "always_nonsmall",
    # baseline performance
    "ln_totalscr88", "ln_totalscr90"
]

df_reg = df.dropna(subset=reg_features + [reg_target]).copy()

X = df_reg[reg_features]
y = df_reg[reg_target]

print("Regression ML dataset shape:", X.shape)

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=123
)

results_reg = {}

def eval_reg_model(name, model):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    mse  = mean_squared_error(y_test, y_pred)   # plain MSE
    rmse = mse ** 0.5                           # convert to RMSE
    r2   = r2_score(y_test, y_pred)

    results_reg[name] = {"RMSE": rmse, "R2": r2}
    print(f"{name:25s}  RMSE = {rmse: .4f}   R² = {r2: .4f}")

# 1. OLS baseline
ols = LinearRegression()
eval_reg_model("OLS (baseline)", ols)

# 2. LASSO
lasso = Pipeline([
    ("scaler", StandardScaler()),
    ("model", Lasso(alpha=0.01, max_iter=10000))
])
eval_reg_model("LASSO (alpha=0.01)", lasso)

# 3. Ridge
ridge = Pipeline([
    ("scaler", StandardScaler()),
    ("model", Ridge(alpha=1.0))
])
eval_reg_model("Ridge (alpha=1.0)", ridge)

# 4. Elastic Net
elastic = Pipeline([
    ("scaler", StandardScaler()),
    ("model", ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000))
])
eval_reg_model("Elastic Net", elastic)

# 5. Random Forest
rf = RandomForestRegressor(
    n_estimators=300,
    max_depth=None,
    random_state=123,
    n_jobs=-1
)
eval_reg_model("Random Forest", rf)

# 6. Gradient Boosting
gbr = GradientBoostingRegressor(
    n_estimators=300,
    learning_rate=0.05,
    max_depth=3,
    random_state=123
)
eval_reg_model("Gradient Boosting", gbr)

# 7. Support Vector Regression
svr = Pipeline([
    ("scaler", StandardScaler()),
    ("model", SVR(kernel="rbf", C=10.0, epsilon=0.1))
])
eval_reg_model("SVR (RBF)", svr)

# 8. k-NN Regression
knn = Pipeline([
    ("scaler", StandardScaler()),
    ("model", KNeighborsRegressor(n_neighbors=7))
])
eval_reg_model("k-NN (k=7)", knn)

print("\n=== Regression summary ===")
for name, metrics in results_reg.items():
    print(f"{name:25s}  RMSE={metrics['RMSE']:.4f}  R²={metrics['R2']:.4f}")

"""Classification ML on absent88_bin and uni_aspire88"""

clf_features = reg_features  # we are using the same predictors

def run_classification(target_col):
    print(f"\n\n===== Classification for {target_col} =====")

    df_clf = df.dropna(subset=clf_features + [target_col]).copy()
    X = df_clf[clf_features]

    # --- handle string labels -> 0/1 ---
    y_raw = df_clf[target_col]

    if y_raw.dtype == "O":  # object / string
        if target_col == "absent88_bin":
            # 1 = frequently absent, 0 = not frequently absent
            y = np.where(
                y_raw.str.contains("Not", case=False, na=False),
                0,
                1
            )
        elif target_col == "uni_aspire88":
            # 1 = aspires to complete university, 0 = does not
            y = np.where(
                y_raw.str.contains("Not", case=False, na=False),
                0,
                1
            )
        else:
            raise ValueError(f"Don't know how to encode {target_col}")
    else:
        # already numeric (0/1)
        y = y_raw.astype(int)

    # ---------------- split & evaluate ----------------
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=123, stratify=y
    )

    results = {}

    def eval_clf_model(name, model):
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)
        if hasattr(model, "predict_proba"):
            y_score = model.predict_proba(X_test)[:, 1]
        else:
            y_score = model.decision_function(X_test)
        auc = roc_auc_score(y_test, y_score)
        results[name] = {"ACC": acc, "AUC": auc}
        print(f"{name:30s}  Acc = {acc: .4f}   AUC = {auc: .4f}")

    # 1. Logistic (L2)
    logit = Pipeline([
        ("scaler", StandardScaler()),
        ("model", LogisticRegression(
            penalty="l2", C=1.0, solver="lbfgs", max_iter=500
        ))
    ])
    eval_clf_model("Logistic Regression (L2)", logit)

    # 2. Random Forest
    rf_clf = RandomForestClassifier(
        n_estimators=300,
        max_depth=None,
        random_state=123,
        n_jobs=-1
    )
    eval_clf_model("Random Forest Classifier", rf_clf)

    # 3. Gradient Boosting
    gbr_clf = GradientBoostingClassifier(
        n_estimators=300,
        learning_rate=0.05,
        max_depth=3,
        random_state=123
    )
    eval_clf_model("Gradient Boosting Classifier", gbr_clf)

    # 4. SVM
    svm_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("model", SVC(kernel="rbf", C=5.0, probability=True))
    ])
    eval_clf_model("SVM (RBF) Classifier", svm_clf)

    # 5. k-NN
    knn_clf = Pipeline([
        ("scaler", StandardScaler()),
        ("model", KNeighborsClassifier(n_neighbors=7))
    ])
    eval_clf_model("k-NN (k=7) Classifier", knn_clf)

    print(f"\nSummary for {target_col}:")
    for name, m in results.items():
        print(f"{name:30s}  Acc={m['ACC']:.4f}  AUC={m['AUC']:.4f}")

run_classification("absent88_bin")
run_classification("uni_aspire88")

"""Unsupervised: PCA + k-means"""

unsup_features = reg_features

df_unsup = df.dropna(subset=unsup_features + ["ln_totalscr92"]).copy()
X_unsup = df_unsup[unsup_features]

# standardise before PCA / k-means
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_unsup)

# PCA to 5 components
pca = PCA(n_components=5, random_state=123)
X_pca = pca.fit_transform(X_scaled)

print("\nExplained variance ratio by PC:")
for i, v in enumerate(pca.explained_variance_ratio_, start=1):
    print(f"PC{i}: {v:.3f}")

# k-means on PCA scores
k = 3  # e.g. 3 clusters
kmeans = KMeans(n_clusters=k, random_state=123, n_init=10)
clusters = kmeans.fit_predict(X_pca)

df_unsup["cluster"] = clusters

print("\nCluster sizes:")
print(df_unsup["cluster"].value_counts())

print("\nCluster means (ln_totalscr92 and class_size92):")
print(df_unsup.groupby("cluster")[["ln_totalscr92", "class_size92"]].mean())